{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ollama Web Rag\n",
    "## Config Web Loader"
   ],
   "id": "cbd1f4ee58e0cf36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:31.299524Z",
     "start_time": "2024-11-10T18:35:31.295438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import PGVector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMETATION\"] = \"python\"\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
    "os.environ[\"OPENAI_API_KEY\"] =  \"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\""
   ],
   "id": "819b314d1a689abf",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Webpages",
   "id": "d6817ba31a28d1b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:34.375909Z",
     "start_time": "2024-11-10T18:35:33.751717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of URLs to load documents from\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "# Load documents from the URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n"
   ],
   "id": "54bb6a62b7e84ee2",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split text into chunks",
   "id": "840fd682a7576271"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:37.229149Z",
     "start_time": "2024-11-10T18:35:37.217374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#todo findout what tiktoken is text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "chunks = text_splitter.split_documents(docs_list)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ],
   "id": "9a921dab0eba83b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 378 chunks\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vector Database",
   "id": "ec597fcb0fa912f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:45.382230Z",
     "start_time": "2024-11-10T18:35:43.539854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\")),\n",
    "    collection_name=\"local-rag-tacoma\"\n",
    ")\n",
    "pgvector = PGVector.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\")),\n",
    "    collection_name=\"local-rag-tacoma\"\n",
    ")\n",
    "print(f\"Vector database created successfully\")"
   ],
   "id": "40cbf18410e78c61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:49.372947Z",
     "start_time": "2024-11-10T18:35:49.360235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "local_model = \"llama3.2:latest\"\n",
    "#local_model = \"granite3-dense:8b\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant.  Your task is to generate 2 \n",
    "    different versions of the give user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on user question, your\n",
    "    goal is to help users overcome some of the limitations of distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    chroma_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=query_prompt\n",
    ")"
   ],
   "id": "4839fcb68b110d1f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Chain",
   "id": "e22bca1a3cc07fd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:54.634841Z",
     "start_time": "2024-11-10T18:35:54.632048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = \"\"\"Answer the question on on the following context: \n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ],
   "id": "2f7dc1b07d6545f2",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:56.734237Z",
     "start_time": "2024-11-10T18:35:56.729816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "ad21018c0871185d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:35:59.392946Z",
     "start_time": "2024-11-10T18:35:59.390668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_with_web(question): \n",
    "    \"\"\"\n",
    "    Chat with the PDF using our chain\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))"
   ],
   "id": "98daf10915e7b59d",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T18:36:07.400702Z",
     "start_time": "2024-11-10T18:36:01.720475Z"
    }
   },
   "cell_type": "code",
   "source": "chat_with_web(\"What is prompt engineering?\")",
   "id": "1a1841d72c67644",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "According to the provided context, Prompt Engineering, also known as In-Context Prompting, refers to methods for communicating with Large Language Models (LLMs) to steer their behavior for desired outcomes without updating the model weights. It is an empirical science that requires heavy experimentation and heuristics, and its effectiveness can vary greatly among different models."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
