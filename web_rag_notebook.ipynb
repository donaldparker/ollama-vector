{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ollama Web Rag\n",
    "## Config Web Loader"
   ],
   "id": "cbd1f4ee58e0cf36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:55:32.905817Z",
     "start_time": "2024-11-11T18:55:32.902536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import PGVector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n"
   ],
   "id": "819b314d1a689abf",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Webpages",
   "id": "d6817ba31a28d1b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:55:37.137048Z",
     "start_time": "2024-11-11T18:55:36.124628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of URLs to load documents from\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "# Load documents from the URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n"
   ],
   "id": "54bb6a62b7e84ee2",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split text into chunks",
   "id": "840fd682a7576271"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:55:41.149880Z",
     "start_time": "2024-11-11T18:55:41.137804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#todo findout what tiktoken is text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "chunks = text_splitter.split_documents(docs_list)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ],
   "id": "9a921dab0eba83b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 378 chunks\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vector Database",
   "id": "ec597fcb0fa912f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:55:53.162950Z",
     "start_time": "2024-11-11T18:55:43.507192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collection_name = \"local-rag\"\n",
    "\n",
    "chroma_db_openai = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\")),\n",
    "    collection_name=f\"{collection_name}-openai\"\n",
    ")\n",
    "\n",
    "chroma_db_nomic = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name= f\"{collection_name}-nomic\"\n",
    ")\n",
    "\n",
    "pgvector_db_openai = PGVector.from_documents(\n",
    "    collection_name=f\"{collection_name}-openai\",\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\")),\n",
    "    use_jsonb=True\n",
    ")\n",
    "pgvector_db_nomic = PGVector.from_documents(\n",
    "    collection_name=f\"{collection_name}-nomic\",\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    use_jsonb=True\n",
    ")\n",
    "\n",
    "print(f\"Vector database created successfully\")"
   ],
   "id": "40cbf18410e78c61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Chain",
   "id": "e22bca1a3cc07fd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:56:09.244115Z",
     "start_time": "2024-11-11T18:56:09.233184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "local_model = \"llama3.2:latest\"\n",
    "#local_model = \"granite3-dense:8b\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant.  Your task is to generate 2 \n",
    "    different versions of the give user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on user question, your\n",
    "    goal is to help users overcome some of the limitations of distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "chroma_retriever_openai = MultiQueryRetriever.from_llm(\n",
    "    chroma_db_openai.as_retriever(),\n",
    "    llm,\n",
    "    prompt=query_prompt\n",
    ")\n",
    "pg_retriever_openai = MultiQueryRetriever.from_llm(\n",
    "    pgvector_db_openai.as_retriever(),\n",
    "    llm,\n",
    "    prompt=query_prompt\n",
    ")\n",
    "chroma_retriever_nomic= MultiQueryRetriever.from_llm(\n",
    "    chroma_db_nomic.as_retriever(),\n",
    "    llm,\n",
    "    prompt=query_prompt\n",
    ")\n",
    "pg_retriever_nomic = MultiQueryRetriever.from_llm(\n",
    "    pgvector_db_nomic.as_retriever(),\n",
    "    llm,\n",
    "    prompt=query_prompt\n",
    ")"
   ],
   "id": "4839fcb68b110d1f",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:43:36.515594Z",
     "start_time": "2024-11-11T18:43:36.512634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = \"\"\"Answer the question on on the following context: \n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ],
   "id": "2f7dc1b07d6545f2",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:56:14.198341Z",
     "start_time": "2024-11-11T18:56:14.194606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chroma_chain_openai = (\n",
    "    {\"context\": chroma_retriever_openai, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pg_chain_openai = (\n",
    "    {\"context\": pg_retriever_openai, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chroma_chain_nomic = (\n",
    "    {\"context\": chroma_retriever_nomic, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pg_chain_nomic = (\n",
    "    {\"context\": pg_retriever_nomic, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "ad21018c0871185d",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:56:18.422279Z",
     "start_time": "2024-11-11T18:56:18.418535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chroma_chat_with_web_openai(question): \n",
    "    \"\"\"\n",
    "    Chat with the PDF using our chain\n",
    "    \"\"\"\n",
    "    return display(Markdown(chroma_chain_openai.invoke(question)))\n",
    "def pg_chat_with_web_openai(question): \n",
    "    \"\"\"\n",
    "    Chat with the PDF using our chain\n",
    "    \"\"\"\n",
    "    return display(Markdown(pg_chain_openai.invoke(question)))\n",
    "def chroma_chat_with_web_nomic(question): \n",
    "    \"\"\"\n",
    "    Chat with the PDF using our chain\n",
    "    \"\"\"\n",
    "    return display(Markdown(chroma_chain_nomic.invoke(question)))\n",
    "def pg_chat_with_web_nomic(question): \n",
    "    \"\"\"\n",
    "    Chat with the PDF using our chain\n",
    "    \"\"\"\n",
    "    return display(Markdown(pg_chain_nomic.invoke(question)))\n"
   ],
   "id": "98daf10915e7b59d",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:56:28.409907Z",
     "start_time": "2024-11-11T18:56:21.759457Z"
    }
   },
   "cell_type": "code",
   "source": "chroma_chat_with_web_openai(\"What is prompt engineering?\")",
   "id": "1a1841d72c67644",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "According to the provided documents, prompt engineering, also known as In-Context Prompting, refers to methods for communicating with LLMs (Large Language Models) to steer their behavior for desired outcomes without updating the model weights. It is an empirical science that requires heavy experimentation and heuristics, as the effect of prompt engineering methods can vary a lot among models."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:56:30.660193Z",
     "start_time": "2024-11-11T18:56:28.418752Z"
    }
   },
   "cell_type": "code",
   "source": "chroma_chat_with_web_nomic(\"What is prompt engineering?\")",
   "id": "968fbeed6dc155b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "According to the provided context, Prompt Engineering (also known as In-Context Prompting) refers to methods for communicating with LLMs to steer their behavior towards desired outcomes without updating the model weights. It is an empirical science that requires heavy experimentation and heuristics, and its effect can vary greatly among models."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:56:44.751486Z",
     "start_time": "2024-11-11T18:56:39.889229Z"
    }
   },
   "cell_type": "code",
   "source": "pg_chat_with_web_openai(\"What is prompt engineering?\")",
   "id": "e7f6924c9d6103d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Prompt engineering refers to methods for how to communicate with a Large Language Model (LLM) to steer its behavior for desired outcomes without updating the model weights. It is an empirical science, and the effect of prompt engineering methods can vary a lot among models, requiring heavy experimentation and heuristics. Prompt engineering focuses on alignment and model steerability."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T18:56:54.972871Z",
     "start_time": "2024-11-11T18:56:51.995232Z"
    }
   },
   "cell_type": "code",
   "source": "pg_chat_with_web_nomic(\"What is prompt engineering?\")",
   "id": "a890c44e62073e40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Based on the provided context, prompt engineering, also known as In-Context Prompting, refers to methods for communicating with LLMs to steer their behavior for desired outcomes without updating the model weights. It involves using a sequence of prefix tokens (prompts) to increase the probability of getting a desired output given input, and can be optimized directly on the embedding space via gradient descent. The goal of prompt engineering is to fine-tune the behavior of LLMs without modifying their underlying weights, allowing for more flexibility and control over their performance."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 118
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
